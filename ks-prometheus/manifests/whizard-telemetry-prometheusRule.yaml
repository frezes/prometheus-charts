apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    app.kubernetes.io/name: whizard-telemetry
    app.kubernetes.io/part-of: whizard-telemetry
    prometheus: k8s
    role: alert-rules
  name: whizard-telemetry-rules
  namespace: kubesphere-monitoring-system
spec:
  groups:
  - name: whizard-telemetry-cluster.rules
    rules:
    - expr: |
        max by (cluster, node, workspace, namespace, pod, qos_class, phase, workload, workload_type) (
                  kube_pod_info{job="kube-state-metrics"}
                * on (cluster, namespace, pod) group_left (qos_class)
                  max by (cluster, namespace, pod, qos_class) (
                    kube_pod_status_qos_class{job="kube-state-metrics"} > 0
                  )
              * on (cluster, namespace, pod) group_left (phase)
                max by (cluster, namespace, pod, phase) (kube_pod_status_phase{job="kube-state-metrics"} > 0)
            * on (cluster, namespace, pod) group_left (workload, workload_type)
              max by (cluster, namespace, pod, workload, workload_type) (
                  label_join(
                    label_join(
                      kube_pod_owner{job="kube-state-metrics",owner_kind!~"ReplicaSet|DaemonSet|StatefulSet|Job"},
                      "workload",
                      "$1",
                      "owner_name"
                    ),
                    "workload_type",
                    "$1",
                    "owner_kind"
                  )
                or
                    kube_pod_owner{job="kube-state-metrics",owner_kind=~"ReplicaSet|DaemonSet|StatefulSet|Job"}
                  * on (cluster, namespace, pod) group_left (workload_type, workload)
                    namespace_workload_pod:kube_pod_owner:relabel
              )
          * on (cluster, namespace) group_left (workspace)
            max by (cluster, namespace, workspace) (kube_namespace_labels{job="kube-state-metrics"})
        )
      record: 'workspace_workload_node:kube_pod_info:'
    - expr: |
        label_replace(
          max by (cluster, node, role, internal_ip) (
                kube_node_info{job="kube-state-metrics"}
              * on (cluster, node) group_left (role)
                max by (cluster, node, role) (
                        (
                            kube_node_role{job="kube-state-metrics",role="worker"}
                          unless ignoring (role)
                            kube_node_role{job="kube-state-metrics",role=~"control-plane|edge"}
                        )
                      or
                        (
                            kube_node_role{job="kube-state-metrics",role="control-plane"}
                          unless ignoring (role)
                            kube_node_role{job="kube-state-metrics",role="edge"}
                        )
                    or
                      kube_node_role{job="kube-state-metrics",role="edge"}
                  or
                    topk by (cluster, node) (
                      1,
                        kube_node_role{job="kube-state-metrics"}
                      unless ignoring (role)
                        (kube_node_role{job="kube-state-metrics",role=~"edge|control-plane|worker|fakenode"})
                    )
                )
            or
                kube_node_info{job="kube-state-metrics"}
              unless on (cluster, node)
                kube_node_role{job="kube-state-metrics"}
          ),
          "host_ip",
          "$1",
          "internal_ip",
          "(.*)"
        )
      record: 'node_role_ip:kube_node_info:'
  - name: whizard-telemetry-node.rules
    rules:
    - expr: |
        sum by (cluster, node) (
            avg by (cluster, instance, namespace, pod) (
              sum without (mode) (
                rate(node_cpu_seconds_total{job="node-exporter",mode!="idle",mode!="iowait",mode!="steal"}[5m])
              )
            )
          * on (cluster, namespace, pod) group_left (node)
            topk by (cluster, namespace, pod) (1, node_namespace_pod:kube_pod_info:)
        )
      record: node:node_cpu_utilization:ratio
    - expr: |
        node:node_cpu_utilization:ratio * node:node_num_cpu:sum
      record: node:node_cpu_utilization:sum
    - expr: |
        node:node_memory_used_bytes:sum / node:node_memory_bytes_total:sum
      record: node:node_memory_utilisation:ratio
    - expr: |
        node:node_memory_bytes_total:sum - node:node_memory_used_bytes:sum
      record: node:node_memory_available_bytes:sum
    - expr: |
        sum by (cluster, node) (
            (
                node_memory_MemTotal_bytes{job="node-exporter"}
              -
                (
                    node_memory_MemAvailable_bytes{job="node-exporter"}
                  or
                    (
                          node_memory_Buffers_bytes{job="node-exporter"} + node_memory_Cached_bytes{job="node-exporter"}
                        +
                          node_memory_MemFree_bytes{job="node-exporter"}
                      +
                        node_memory_Slab_bytes{job="node-exporter"}
                    )
                )
            )
          * on (cluster, namespace, pod) group_left (node)
            topk by (cluster, namespace, pod) (1, node_namespace_pod:kube_pod_info:)
        )
      record: node:node_memory_used_bytes:sum
    - expr: |
        sum by (cluster, node) (
            node_memory_MemTotal_bytes{job="node-exporter"}
          * on (cluster, namespace, pod) group_left (node)
            topk by (cluster, namespace, pod) (1, node_namespace_pod:kube_pod_info:)
        )
      record: node:node_memory_bytes_total:sum
    - expr: |
        node:node_device_filesystem_used_bytes:sum / node:node_device_filesystem_bytes_total:sum
      record: node:node_device_filesystem_utilisation:ratio
    - expr: |
        sum by (cluster, node, device) (
              max by (cluster, namespace, pod, instance, device) (
                  node_filesystem_avail_bytes{device=~"/dev/.*",device!~"/dev/loop\\d+", job="node-exporter"}
              )
          * on (cluster, namespace, pod) group_left (node)
            topk by (cluster, namespace, pod) (1, node_namespace_pod:kube_pod_info:)
        )
      record: node:node_device_filesystem_available_bytes:sum
    - expr: |
        sum by (cluster, node, device) (
              max by (cluster, namespace, pod, instance, device) (
                  node_filesystem_size_bytes{device=~"/dev/.*",device!~"/dev/loop\\d+", job="node-exporter"}
                -
                  node_filesystem_avail_bytes{device=~"/dev/.*",device!~"/dev/loop\\d+", job="node-exporter"}
              )
          * on (cluster, namespace, pod) group_left (node)
            topk by (cluster, namespace, pod) (1, node_namespace_pod:kube_pod_info:)
        )
      record: node:node_device_filesystem_used_bytes:sum
    - expr: |
        sum by (cluster, node, device) (
              max by (cluster, namespace, pod, instance, device) (
                  node_filesystem_size_bytes{device=~"/dev/.*",device!~"/dev/loop\\d+", job="node-exporter"}
              )
          * on (cluster, namespace, pod) group_left (node)
            topk by (cluster, namespace, pod) (1, node_namespace_pod:kube_pod_info:)
        )
      record: node:node_device_filesystem_bytes_total:sum
    - expr: |
        node:node_filesystem_used_bytes:sum / node:node_filesystem_bytes_total:sum
      record: node:node_filesystem_utilisation:ratio
    - expr: |
        sum by (cluster, node)(node:node_device_filesystem_available_bytes:sum)
      record: node:node_filesystem_available_bytes:sum
    - expr: |
        sum by (cluster, node)(node:node_device_filesystem_used_bytes:sum)
      record: node:node_filesystem_used_bytes:sum
    - expr: |
        sum by (cluster, node)(node:node_device_filesystem_bytes_total:sum)
      record: node:node_filesystem_bytes_total:sum
    - expr: |
        node:node_pod_running_total:sum / node:node_pod_quota:sum
      record: node:node_pod_utilisation:ratio
    - expr: |
        count by(cluster, node) (
            node_namespace_pod:kube_pod_info:
            unless on (cluster, namespace, pod)
            (kube_pod_status_phase{job="kube-state-metrics", phase=~"Failed|Pending|Unknown|Succeeded"} > 0)
        )
      record: node:node_pod_running_total:sum
    - expr: |
        sum by (cluster, node) (
            sum by (cluster, namespace, pod) (kube_pod_status_scheduled{job="kube-state-metrics"} > 0)
          * on (cluster, namespace, pod) group_left (node)
            node_namespace_pod:kube_pod_info:
        )
      record: node:node_pod_total:sum
    - expr: |
        sum by (cluster, node) (kube_node_status_allocatable{job="kube-state-metrics",resource="pods"})
      record: node:node_pod_quota:sum
    - expr: |
        count by (cluster, node) (
                  node_namespace_pod:kube_pod_info:{node!=""}
                unless on (pod, namespace, cluster)
                  (kube_pod_status_phase{job="kube-state-metrics",phase="Succeeded"} > 0)
              unless on (pod, namespace, cluster)
                (
                    (kube_pod_status_ready{condition="true",job="kube-state-metrics"} > 0)
                  and on (pod, namespace, cluster)
                    (kube_pod_status_phase{job="kube-state-metrics",phase="Running"} > 0)
                )
            unless on (cluster, pod, namespace)
              kube_pod_container_status_waiting_reason{job="kube-state-metrics",reason="ContainerCreating"} > 0
        )
        /
        count by (cluster, node) (
              node_namespace_pod:kube_pod_info:{node!=""}
            unless on (pod, namespace, cluster)
              kube_pod_status_phase{job="kube-state-metrics",phase="Succeeded"} > 0
        )
      record: node:pod_abnormal_utilisation:ratio
    - expr: |
        sum by (cluster, node)(node_load1{job="node-exporter"} * on (cluster, namespace, pod) group_left (node) topk by (cluster, namespace, pod) (1, node_namespace_pod:kube_pod_info:)) / node:node_num_cpu:sum
      record: node:node_load1_per_cpu:ratio
    - expr: |
        sum by (cluster, node)(node_load5{job="node-exporter"} * on (cluster, namespace, pod) group_left (node) topk by (cluster, namespace, pod) (1, node_namespace_pod:kube_pod_info:)) / node:node_num_cpu:sum
      record: node:node_load5_per_cpu:ratio
    - expr: |
        sum by (cluster, node)(node_load15{job="node-exporter"} * on (cluster, namespace, pod) group_left (node) topk by (cluster, namespace, pod) (1, node_namespace_pod:kube_pod_info:)) / node:node_num_cpu:sum
      record: node:node_load15_per_cpu:ratio
    - expr: |
        sum by (cluster, node) (
            sum by (cluster, instance, namespace, pod) (
              irate(node_disk_read_bytes_total{job="node-exporter"}[5m])
            )
          * on (cluster, namespace, pod) group_left (node)
            topk by (cluster, namespace, pod) (1, node_namespace_pod:kube_pod_info:)
        )
      record: node:data_volume_iops_reads:sum
    - expr: |
        sum by (cluster, node) (
            sum by (cluster, instance, namespace, pod) (
              irate(node_disk_writes_completed_total{job="node-exporter"}[5m])
            )
          * on (cluster, namespace, pod) group_left (node)
            topk by (cluster, namespace, pod) (1, node_namespace_pod:kube_pod_info:)
        )
      record: node:data_volume_iops_writes:sum
    - expr: |
        sum by (cluster, node) (
            sum by (cluster, instance, namespace, pod) (
              irate(node_disk_read_bytes_total{job="node-exporter"}[5m])
            )
          * on (cluster, namespace, pod) group_left (node)
            topk by (cluster, namespace, pod) (1, node_namespace_pod:kube_pod_info:)
        )
      record: node:data_volume_throughput_read_bytes:sum_irate
    - expr: |
        sum by (cluster, node) (
            sum by (cluster, instance, namespace, pod) (
              irate(node_disk_written_bytes_total{job="node-exporter"}[5m])
            )
          * on (cluster, namespace, pod) group_left (node)
            topk by (cluster, namespace, pod) (1, node_namespace_pod:kube_pod_info:)
        )
      record: node:data_volume_throughput_written_bytes:sum_irate
    - expr: |
        node:node_inodes_used_total:sum / node:node_inodes_total:sum
      record: node:node_inodes_utilisation:ratio
    - expr: |
        sum by (cluster, node) (
            sum by (cluster, instance, namespace, pod) (
              node_filesystem_files{job="node-exporter", device=~"/dev/.*",device!~"/dev/loop\\d+"}
            )
          * on (cluster, namespace, pod) group_left (node)
            topk by (cluster, namespace, pod) (1, node_namespace_pod:kube_pod_info:)
        )
      record: node:node_inodes_total:sum
    - expr: |
        sum by (cluster, node) (
            sum by (cluster, instance, namespace, pod) (
              node_filesystem_files{job="node-exporter", device=~"/dev/.*",device!~"/dev/loop\\d+"} - node_filesystem_files_free{job="node-exporter", device=~"/dev/.*",device!~"/dev/loop\\d+"}
            )
          * on (cluster, namespace, pod) group_left (node)
            topk by (cluster, namespace, pod) (1, node_namespace_pod:kube_pod_info:)
        )
      record: node:node_inodes_used_total:sum
    - expr: |
        sum by (cluster, node) (
            sum by (cluster, instance, namespace, pod) (
              irate(node_network_transmit_bytes_total{job="node-exporter", device!~"veth.+"}[5m])
            )
          * on (cluster, namespace, pod) group_left (node)
            topk by (cluster, namespace, pod) (1, node_namespace_pod:kube_pod_info:)
        )
      record: node:node_net_transmit_bytes:sum_irate
    - expr: |
        sum by (cluster, node) (
            sum by (cluster, instance, namespace, pod) (
              irate(node_network_receive_bytes_total{job="node-exporter", device!~"veth.+"}[5m])
            )
          * on (cluster, namespace, pod) group_left (node)
            topk by (cluster, namespace, pod) (1, node_namespace_pod:kube_pod_info:)
        )
      record: node:node_net_receive_bytes:sum_irate
  - name: whizard-telemetry-namespace.rules
    rules:
    - expr: |
        sum by (cluster, namespace, workload, workload_type) (
            sum by (cluster, namespace, pod) (
              node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate
            )
          * on (cluster, namespace, pod) group_left (workload, workload_type)
            workspace_workload_node:kube_pod_info:
        )
      record: namespace:workload_cpu_usage:sum
    - expr: |
        sum by (cluster, namespace, workload, workload_type) (
            sum by (cluster, namespace, pod) (
              container_memory_usage_bytes{job="kubelet", metrics_path="/metrics/cadvisor", image!=""}
            )
          * on (cluster, namespace, pod) group_left (workload, workload_type)
            workspace_workload_node:kube_pod_info:
        )
      record: namespace:workload_memory_usage:sum
    - expr: |
        sum by (cluster, namespace, workload, workload_type) (
            sum by (cluster, namespace, pod) (
              node_namespace_pod_container:container_memory_working_set_bytes
            )
          * on (cluster, namespace, pod) group_left (workload, workload_type)
            workspace_workload_node:kube_pod_info:
        )
      record: namespace:workload_memory_wo_cache_usage:sum
    - expr: |
        sum by (cluster, namespace, workload, workload_type) (
            sum by (cluster, namespace, pod) (
              irate(container_network_receive_bytes_total{job="kubelet", metrics_path="/metrics/cadvisor", image!=""}[5m])
            )
          * on (cluster, namespace, pod) group_left (workload, workload_type)
            workspace_workload_node:kube_pod_info:
        )
      record: namespace:workload_net_receive_bytes:sum_irate
    - expr: |
        sum by (cluster, namespace, workload, workload_type) (
            sum by (cluster, namespace, pod) (
              irate(container_network_transmit_bytes_total{job="kubelet", metrics_path="/metrics/cadvisor", image!=""}[5m])
            )
          * on (cluster, namespace, pod) group_left (workload, workload_type)
            workspace_workload_node:kube_pod_info:
        )
      record: namespace:workload_net_transmit_bytes:sum_irate
    - expr: |
        label_replace(sum(kube_daemonset_status_number_unavailable{job="kube-state-metrics"}) by (daemonset, namespace, cluster) / sum(kube_daemonset_status_desired_number_scheduled{job="kube-state-metrics"}) by (daemonset, namespace,cluster), "workload", "$1", "daemonset", "(.*)")
      labels:
        workload_type: daemonset
      record: namespace:workload_unavailable_replicas:ratio
    - expr: |
        label_replace(sum(kube_deployment_status_replicas_unavailable{job="kube-state-metrics"}) by (deployment, namespace, cluster) / sum(kube_deployment_spec_replicas{job="kube-state-metrics"}) by (deployment, namespace, cluster), "workload", "$1", "deployment", "(.*)")
      labels:
        workload_type: deployment
      record: namespace:workload_unavailable_replicas:ratio
    - expr: |
        label_replace(1 - sum(kube_statefulset_status_replicas_ready{job="kube-state-metrics"}) by (statefulset, namespace, cluster) / sum(kube_statefulset_status_replicas{job="kube-state-metrics"}) by (statefulset, namespace, cluster), "workload", "$1", "statefulset", "(.*)")
      labels:
        workload_type: statefulset
      record: namespace:workload_unavailable_replicas:ratio
  - name: whizard-telemetry-apiserver.rules
    rules:
    - expr: |
        sum by(cluster) (irate(apiserver_request_total{job="apiserver"}[5m]))
      record: apiserver:apiserver_request_total:sum_irate
    - expr: |
        sum by (cluster, verb)(irate(apiserver_request_total{job="apiserver"}[5m]))
      record: apiserver:apiserver_request_total:sum_verb_irate
    - expr: |
        sum by(cluster) (irate(apiserver_request_duration_seconds_sum{job="apiserver",subresource!="log", verb!~"LIST|WATCH|WATCHLIST|PROXY|CONNECT"}[5m])) / sum by(cluster) (irate(apiserver_request_duration_seconds_count{job="apiserver", subresource!="log",verb!~"LIST|WATCH|WATCHLIST|PROXY|CONNECT"}[5m]))
      record: apiserver:apiserver_request_duration:avg
    - expr: "sum by (cluster, verb)(irate(apiserver_request_duration_seconds_sum{job=\"apiserver\",subresource!=\"log\", verb!~\"LIST|WATCH|WATCHLIST|PROXY|CONNECT\"}[5m]))  / sum by (cluster, verb)(irate(apiserver_request_duration_seconds_count{job=\"apiserver\", subresource!=\"log\",verb!~\"LIST|WATCH|WATCHLIST|PROXY|CONNECT\"}[5m])) \n"
      record: apiserver:apiserver_request_duration:avg_by_verb
  - name: whizard-telemetry-etcd.rules
    rules:
    - expr: |
        sum by(cluster) (up{job=~".*etcd.*"} == 1)
      record: etcd:up:sum
    - expr: |
        sum(label_replace(sum(changes(etcd_server_leader_changes_seen_total{job=~".*etcd.*"}[1h])) by (instance, cluster), "node", "$1", "instance", "(.*):.*")) by (node, cluster)
      record: etcd:etcd_server_leader_changes_seen:sum_changes
    - expr: |
        sum(label_replace(sum(irate(etcd_server_proposals_failed_total{job=~".*etcd.*"}[5m])) by (instance, cluster), "node", "$1", "instance", "(.*):.*")) by (node, cluster)
      record: etcd:etcd_server_proposals_failed:sum_irate
    - expr: |
        sum(label_replace(sum(irate(etcd_server_proposals_applied_total{job=~".*etcd.*"}[5m])) by (instance, cluster), "node", "$1", "instance", "(.*):.*")) by (node, cluster)
      record: etcd:etcd_server_proposals_applied:sum_irate
    - expr: |
        sum(label_replace(sum(irate(etcd_server_proposals_committed_total{job=~".*etcd.*"}[5m])) by (instance, cluster), "node", "$1", "instance", "(.*):.*")) by (node, cluster)
      record: etcd:etcd_server_proposals_committed:sum_irate
    - expr: |
        sum(label_replace(sum(etcd_server_proposals_pending{job=~".*etcd.*"}) by (instance, cluster), "node", "$1", "instance", "(.*):.*")) by (node, cluster)
      record: etcd:etcd_server_proposals_pending:sum
    - expr: |
        sum(label_replace(etcd_mvcc_db_total_size_in_bytes{job=~".*etcd.*"},"node", "$1", "instance", "(.*):.*")) by (node, cluster)
      record: etcd:etcd_mvcc_db_total_size:sum
    - expr: |
        sum(label_replace(sum(irate(etcd_network_client_grpc_received_bytes_total{job=~".*etcd.*"}[5m])) by (instance, cluster), "node", "$1", "instance", "(.*):.*")) by (node, cluster)
      record: etcd:etcd_network_client_grpc_received_bytes:sum_irate
    - expr: |
        sum(label_replace(sum(irate(etcd_network_client_grpc_sent_bytes_total{job=~".*etcd.*"}[5m])) by (instance, cluster), "node", "$1", "instance", "(.*):.*")) by (node, cluster)
      record: etcd:etcd_network_client_grpc_sent_bytes:sum_irate
    - expr: |
        sum(label_replace(sum(irate(grpc_server_started_total{job=~".*etcd.*",grpc_type="unary"}[5m])) by (instance, cluster), "node", "$1", "instance", "(.*):.*")) by (node, cluster)
      record: etcd:grpc_server_started:sum_irate
    - expr: |
        sum(label_replace(sum(irate(grpc_server_handled_total{job=~".*etcd.*",grpc_type="unary",grpc_code!="OK"}[5m])) by (instance, cluster), "node", "$1", "instance", "(.*):.*")) by (node, cluster)
      record: etcd:grpc_server_handled:sum_irate
    - expr: |
        sum(label_replace(sum(irate(grpc_server_msg_received_total{job=~".*etcd.*"}[5m])) by (instance, cluster), "node", "$1", "instance", "(.*):.*")) by (node, cluster)
      record: etcd:grpc_server_msg_received:sum_irate
    - expr: |
        sum(label_replace(sum(irate(grpc_server_msg_sent_total{job=~".*etcd.*"}[5m])) by (instance, cluster), "node", "$1", "instance", "(.*):.*")) by (node, cluster)
      record: etcd:grpc_server_msg_sent:sum_irate
    - expr: |
        sum(label_replace(sum(irate(etcd_disk_wal_fsync_duration_seconds_sum{job=~".*etcd.*"}[5m])) by (instance, cluster) / sum(irate(etcd_disk_wal_fsync_duration_seconds_count{job=~".*etcd.*"}[5m])) by (instance, cluster), "node", "$1", "instance", "(.*):.*")) by (node, cluster)
      record: etcd:etcd_disk_wal_fsync_duration:avg
    - expr: |
        sum(label_replace(sum(irate(etcd_disk_backend_commit_duration_seconds_sum{job=~".*etcd.*"}[5m])) by (instance, cluster) / sum(irate(etcd_disk_backend_commit_duration_seconds_count{job=~".*etcd.*"}[5m])) by (instance, cluster), "node", "$1", "instance", "(.*):.*")) by (node, cluster)
      record: etcd:etcd_disk_backend_commit_duration:avg
    - expr: |
        histogram_quantile(0.99, sum(label_replace(sum(irate(etcd_disk_wal_fsync_duration_seconds_bucket{job=~".*etcd.*"}[5m])) by (instance, le, cluster), "node", "$1", "instance", "(.*):.*")) by (node, le, cluster))
      labels:
        quantile: "0.99"
      record: etcd:etcd_disk_wal_fsync_duration:histogram_quantile
    - expr: |
        histogram_quantile(0.9, sum(label_replace(sum(irate(etcd_disk_wal_fsync_duration_seconds_bucket{job=~".*etcd.*"}[5m])) by (instance, le, cluster), "node", "$1", "instance", "(.*):.*")) by (node, le, cluster))
      labels:
        quantile: "0.9"
      record: etcd:etcd_disk_wal_fsync_duration:histogram_quantile
    - expr: |
        histogram_quantile(0.5, sum(label_replace(sum(irate(etcd_disk_wal_fsync_duration_seconds_bucket{job=~".*etcd.*"}[5m])) by (instance, le, cluster), "node", "$1", "instance", "(.*):.*")) by (node, le, cluster))
      labels:
        quantile: "0.5"
      record: etcd:etcd_disk_wal_fsync_duration:histogram_quantile
    - expr: |
        histogram_quantile(0.99, sum(label_replace(sum(irate(etcd_disk_backend_commit_duration_seconds_bucket{job=~".*etcd.*"}[5m])) by (instance, le, cluster), "node", "$1", "instance", "(.*):.*")) by (node, le, cluster))
      labels:
        quantile: "0.99"
      record: etcd:etcd_disk_backend_commit_duration:histogram_quantile
    - expr: |
        histogram_quantile(0.9, sum(label_replace(sum(irate(etcd_disk_backend_commit_duration_seconds_bucket{job=~".*etcd.*"}[5m])) by (instance, le, cluster), "node", "$1", "instance", "(.*):.*")) by (node, le, cluster))
      labels:
        quantile: "0.9"
      record: etcd:etcd_disk_backend_commit_duration:histogram_quantile
    - expr: |
        histogram_quantile(0.5, sum(label_replace(sum(irate(etcd_disk_backend_commit_duration_seconds_bucket{job=~".*etcd.*"}[5m])) by (instance, le, cluster), "node", "$1", "instance", "(.*):.*")) by (node, le, cluster))
      labels:
        quantile: "0.5"
      record: etcd:etcd_disk_backend_commit_duration:histogram_quantile
  - name: whizard-telemetry-kube-scheduler.rules
    rules:
    - expr: |
        histogram_quantile(0.99, sum(rate(scheduler_e2e_scheduling_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod, result))
      labels:
        quantile: "0.99"
      record: cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile
    - expr: |
        histogram_quantile(0.99, sum(rate(scheduler_scheduling_attempt_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod, result))
      labels:
        quantile: "0.99"
      record: cluster_quantile:scheduler_scheduling_attempt_duration_seconds:histogram_quantile
    - expr: |
        histogram_quantile(0.9, sum(rate(scheduler_e2e_scheduling_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod, result))
      labels:
        quantile: "0.9"
      record: cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile
    - expr: |
        histogram_quantile(0.9, sum(rate(scheduler_scheduling_attempt_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod, result))
      labels:
        quantile: "0.9"
      record: cluster_quantile:scheduler_scheduling_attempt_duration_seconds:histogram_quantile
    - expr: |
        histogram_quantile(0.5, sum(rate(scheduler_e2e_scheduling_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod, result))
      labels:
        quantile: "0.5"
      record: cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile
    - expr: |
        histogram_quantile(0.5, sum(rate(scheduler_scheduling_attempt_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod, result))
      labels:
        quantile: "0.5"
      record: cluster_quantile:scheduler_scheduling_attempt_duration_seconds:histogram_quantile
    - expr: |
        sum by(cluster) (rate(scheduler_e2e_scheduling_duration_seconds_sum{job="kube-scheduler"}[5m])) / sum by(cluster) (rate(scheduler_e2e_scheduling_duration_seconds_count{job="kube-scheduler"}[5m]))
      record: cluster:scheduler_e2e_scheduling_duration_seconds:avg
    - expr: |
        sum by(cluster) (rate(scheduler_scheduling_attempt_duration_seconds_sum{job="kube-scheduler"}[5m])) / sum by(cluster) (rate(scheduler_scheduling_attempt_duration_seconds_count{job="kube-scheduler"}[5m]))
      record: cluster:scheduler_scheduling_attempt_duration_seconds:avg
